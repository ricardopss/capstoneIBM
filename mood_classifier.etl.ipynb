{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskKYEhxDVR7",
        "colab_type": "text"
      },
      "source": [
        "# Importing Dependencies & extracting dataset\n",
        "----\n",
        "This part of the process deals with the _Extraction_ part of the ***E***TL. Firstly, the needed modules will be imported and then the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-1LqXJ-DXib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5bfd9e39-4451-45b0-9f0b-42413f9fb058"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from keras.preprocessing import sequence as seq\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN2EPRakELdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "04c93168-2c6b-420a-8511-c10f8f31042c"
      },
      "source": [
        "### In case, one wanted to upload the files to a File Storage (in case Google Drive)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlEkmzOTEbBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82b31485-b114-4976-ed11-ed96fd7951c1"
      },
      "source": [
        "### Some commands to create directory structure and rename file - already executed in the Data Exploration Notebook\n",
        "# %cd /content/gdrive/My\\ Drive\n",
        "# !rm -r projects\n",
        "# !mkdir projects\n",
        "# !mkdir projects/capstone\n",
        "# !mkdir projects/capstone/checkpoints\n",
        "%cd /content/gdrive/My\\ Drive/projects/capstone"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/projects/capstone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5m5kytAEoB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Already executed in the Data Exploration Notebook\n",
        "# !rm trainingandtestdata.zip -f\n",
        "# !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip \n",
        "# !unzip trainingandtestdata.zip\n",
        "# !mv training.1600000.processed.noemoticon.csv sentiment_analysis_trainingset.csv"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7q3aFxAFFpw",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset into Pandas framework:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqb0Gt2yEzst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nameColumns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"post\"]\n",
        "\n",
        "fullDataset = pd.read_csv('sentiment_analysis_trainingset.csv', \n",
        "                          header=None, \n",
        "                          names=nameColumns, \n",
        "                          encoding='latin1', \n",
        "                          engine='python')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5374S5YFB5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "e8cc5f66-3a79-497c-804e-375a2fc2cb8f"
      },
      "source": [
        "print(fullDataset.info())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count    Dtype \n",
            "---  ------     --------------    ----- \n",
            " 0   sentiment  1600000 non-null  int64 \n",
            " 1   id         1600000 non-null  int64 \n",
            " 2   date       1600000 non-null  object\n",
            " 3   query      1600000 non-null  object\n",
            " 4   user       1600000 non-null  object\n",
            " 5   post       1600000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ1o2ZRoFSFM",
        "colab_type": "text"
      },
      "source": [
        "## Initial sampling\n",
        "In order to gain time-efficiency, a random sample was extracted from the 1.6MM datapoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyQBo5wqFTRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "f01c3d7d-9b30-40ca-cf5e-c98ee40c37a1"
      },
      "source": [
        "sampleDataset = fullDataset.sample(frac=0.1, replace = None, random_state=300)\n",
        "sampleDataset.index = range(len(sampleDataset))\n",
        "sampleDataset.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count   Dtype \n",
            "---  ------     --------------   ----- \n",
            " 0   sentiment  160000 non-null  int64 \n",
            " 1   id         160000 non-null  int64 \n",
            " 2   date       160000 non-null  object\n",
            " 3   query      160000 non-null  object\n",
            " 4   user       160000 non-null  object\n",
            " 5   post       160000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 7.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fhRflvuFiuF",
        "colab_type": "text"
      },
      "source": [
        "# Tranformations\n",
        "---\n",
        "In this part, we will run the necessary data transformations (E***T***L):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bQ6ltGHg4s",
        "colab_type": "text"
      },
      "source": [
        "## (re)tagging sentiments\n",
        "Ensure that inputs labels will be `0` or `1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_04GoQhfIDIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampleDatasetLabels = sampleDataset.sentiment.values\n",
        "sampleDatasetLabels[sampleDatasetLabels == 4] = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adFsmfKUGE-y",
        "colab_type": "text"
      },
      "source": [
        "## tweets text treatment\n",
        "Here we will treat the Twitter text posts, to remove all sort of noise and non-essencial for our analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvj7bKHOFiE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TweetTreament(stringTweet):\n",
        "    tweet = re.sub(r\"https?://[a-zA-Z0-9./]+\", ' ', stringTweet)  # remove the URL links\n",
        "    tweet = re.sub(r\"(w|W){3}[a-zA-Z0-9./]+\", ' ', tweet)         # remove the www references\n",
        "    tweet = re.sub(r\"@[a-zA-Z0-9]+\", ' ', tweet)                  # remove references from accounts/profiles, using @ \n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)                  # keep only letters & punctuations marks\n",
        "    tweet = re.sub(r\" +\", ' ', tweet).strip()                     # remove potential excess of whitespaces before, in and after\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "cleanedTweets = [TweetTreament(t) for t in sampleDataset.post]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nseS2fOfGPTy",
        "colab_type": "text"
      },
      "source": [
        "## tokenization\n",
        "Tokenization is the process of converting each word in a text to an unique number (integer), called tokens. This is a common task in NLP, since computers learn words from numbers, not characters. Luckily, TensorFlow support us with all necessary methods.\n",
        "\n",
        "The method `SubwordTextEncoder.build_from_corpus()` allows us to build a vocabulary using the universe of words in our dataset. \n",
        "\n",
        "The `.encode()` method convert each word in the Tweet into a token. This process is invertible by using `.decode()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Vm6_u4GUMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "targetVocab = 2**16\n",
        "\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "                                          cleanedTweets, \n",
        "                                          target_vocab_size=targetVocab)\n",
        "\n",
        "tokenizedData = [tokenizer.encode(sentence) for sentence in cleanedTweets]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9W7rmkpGfL0",
        "colab_type": "text"
      },
      "source": [
        "> **point of decision:** the vocabulary size is a critical variable, as a large vocabulary size will result in a high number of word embedding parameters in the model, and therefore a large storage or memory. \n",
        "> Searching in Google, we see that a average Englisch speaker knows around 20.000-40.000 words. For our experiment, we will use the `targetVocab = 2**16`, or the most 65.536 important words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZucAXkkQGUox",
        "colab_type": "text"
      },
      "source": [
        "## padding\n",
        "Since we will use a Convolutional Neural Network (CNN)  model, padding is required and it improves the model performance.\n",
        "\n",
        "Some tweets have more words than others, and this is where the padding is necessary.\n",
        "\n",
        "We need to have the inputs with the same size, so padding represents the process of normalizing the length for each tokenized tweet by completing it with `0`.\n",
        "\n",
        "Luckily, Keras `sequence.pad_sequences()` does this task for us. For common length (`maxLength`) we will use the maximum length found in our sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpyTpfOxI0D7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46a0f5fd-d5bf-4027-fd22-7284f59129b9"
      },
      "source": [
        "maxLength = max([len(p) for p in tokenizedData])\n",
        "\n",
        "paddedData = tf.keras.preprocessing.sequence.pad_sequences(tokenizedData,\n",
        "                                                           value=0,\n",
        "                                                           padding=\"pre\",\n",
        "                                                           maxlen=maxLength)\n",
        "print(maxLength)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUCmrcsrI3zd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c3b5b816-71f7-4d71-f1ea-a9dee3ef30de"
      },
      "source": [
        "paddedData[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,   774,  5919,\n",
              "         589,   208,    86, 53975], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnPngg82I19M",
        "colab_type": "text"
      },
      "source": [
        "# Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtUS4PiI-l_",
        "colab_type": "text"
      },
      "source": [
        "## Spliting data into training/testing set\n",
        "As best practice, we will split our dataset in two groups: training set (80%) and testing set (20%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPG_o4zkJH9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "b7e884ae-2fca-4f5b-9d00-68fced65b892"
      },
      "source": [
        "testSize = int(len(paddedData) * 0.2)\n",
        "print('The testing set has {0} datapoints\\n'.format(testSize))\n",
        "\n",
        "testInputs = paddedData[-testSize:]\n",
        "testLabels = sampleDatasetLabels[-testSize:]\n",
        "trainInputs = paddedData[:-testSize]\n",
        "trainLabels = sampleDatasetLabels[:-testSize]\n",
        "\n",
        "### Checking the shape of the model's inputs and labels \n",
        "print('Size of testing Inputs:', len(testInputs))\n",
        "print('Size of testing Labels:', len(testLabels))\n",
        "print('\\nSize of training Inputs:', len(trainInputs))\n",
        "print('Size of training Inputs:', len(trainLabels))\n",
        "print('\\nFirst observation of training label:', trainLabels[0])\n",
        "print('\\nFirst observation of training inputs:', trainInputs[0])\n",
        "print('\\nTraining inputs shape:', trainInputs[0].shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The testing set has 32000 datapoints\n",
            "\n",
            "Size of testing Inputs: 32000\n",
            "Size of testing Labels: 32000\n",
            "\n",
            "Size of training Inputs: 128000\n",
            "Size of training Inputs: 128000\n",
            "\n",
            "First observation of training label: 1\n",
            "\n",
            "First observation of training inputs: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0   774  5919   589   208    86 53975]\n",
            "\n",
            "Training inputs shape (58,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}